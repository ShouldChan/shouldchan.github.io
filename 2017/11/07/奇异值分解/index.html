<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
  <meta name="viewport" content="width=932" >
  <meta name="MobileOptimized" content="932" >
  <meta name="generator" content="ShouldChan">
  <title>奇异值分解</title>

  <link rel="stylesheet" type="text/css" href="/css/normalize.css">
  <link rel="stylesheet" type="text/css" href="/css/awe.css">
  <link rel="stylesheet" type="text/css" href="/css/social-likes.css">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="">

  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
  <script src="/js/imgLiquid-min.js"></script>
  <script src="/js/social-likes.min.js"></script>
</head>

<body>
  <div class="main-container">
    

<div class="box profile">
  <div class="heading" style="background: #3e5354 url(/images/bg-heading.jpg) no-repeat;">
    <div class="profile-img">
      <a href="//shouldchan.github.io/"><img src="/images/img-profile.jpg" width="154" height="150" alt="image description"></a>
    </div>
    <strong class="user"><a href="/">ShouldChan</a></strong>
    <span class="locate">Suzhou</span>
  </div>
  <div class="setting">
    <a href="//shouldchan.github.io/" class="btn-profile">Profile</a>
    <a href="/" class="btn-photos">Photos</a>
    <a href="/" class="btn-follow">Follow me</a>
  </div>
</div>

    
<div class="two-columns post-page">
  <div class="content">

    <div class="post box nocover">
      <em class="date"><span>07</span>Nov</em>
      <h2 class="post-title"><a href="/2017/11/07/奇异值分解/">奇异值分解</a></h2>
      <div class="post-content"><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>奇异值分解（Singular Value Decomposition，SVD）是矩阵分解的一种，是一种具有物理意义的方法。任意矩阵都可以进行奇异值分解。</p>
<h1 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h1><p>假设存在一个m\times n的矩阵A，A将n维空间中的向量映射到k（k&lt;=m）空间中，k=Rank(A)，即矩阵A的秩。</p>
<p>设n维空间中有一组满足经A变换后还是正交的正交基：<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_1.jpg" alt="1"></p>
<p>则A矩阵将这组基映射为：<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_2.jpg" alt="2"></p>
<p>使他们两两正交，则<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_3.jpg" alt="3"></p>
<p>令下式=0<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_4.jpg" alt="4"></p>
<p>如果正交基v选A^TA的特征向量，因为A^TA是对称阵，v之间两两正交，那么<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_5.jpg" alt="5"></p>
<p>这样就找到了正交基并且映射后还是正交基，将映射后的正交基单位化：<br>因为<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_6.jpg" alt="6"></p>
<p>所以<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_7.jpg" alt="7"></p>
<h2 id="这里插播一条性质："><a href="#这里插播一条性质：" class="headerlink" title="这里插播一条性质："></a>这里插播一条性质：</h2><p>if i等于j, v_i\cdot v_j = 1;<br>else if i不等于j, v_i\cdot v_j = 0。</p>
<p>所以取单位向量<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_8.jpg" alt="8"></p>
<p>由此可得<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_9.jpg" alt="9"></p>
<p>当k &lt; i &lt;= m时，对u_1，u<em>2，…，uk进行扩展u</em>(k+1),…,u_m，使得u_1，u_2，…，u_m为m维空间中的一组正交基，即<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_10.jpg" alt="10"></p>
<p>同样的，对v1，v2，…，vk进行扩展v(k+1),…,vn（这n-k个向量存在于A的零空间中，即Ax=0的解空间的基），使得v1，v2，…，vn为n维空间中的一组正交基，即<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_11.jpg" alt="11"></p>
<p>则可得<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_12.jpg" alt="12"></p>
<p>继而可以得到A矩阵的奇异值分解：<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_13.jpg" alt="13"><br>其中，<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_14.jpg" alt="14"></p>
<h1 id="满秩分解"><a href="#满秩分解" class="headerlink" title="满秩分解"></a>满秩分解</h1><p>现在可以来对A矩阵的映射过程进行分析了：如果在n维空间中找到一个（超）矩形，其边都落在A’A的特征向量的方向上，那么经过A变换后的形状仍然为（超）矩形！<br>vi为A’A的特征向量，称为A的右奇异向量，ui=Avi实际上为AA’的特征向量，称为A的左奇异向量。下面利用SVD证明文章一开始的满秩分解：<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_15.jpg" alt="1"></p>
<p>利用矩阵分块乘法展开得：<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_16.jpg" alt="2"></p>
<p>可以看到第二项为0，所以有<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_17.jpg" alt="3"></p>
<p>令<br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_18.jpg" alt="4"><br><img src="https://raw.githubusercontent.com/ShouldChan/ImageStore/master/blog_image/svd_19.jpg" alt="5"><br>则A=XY即是A的满秩分解。</p>
</div>

      <div class="bottom-panel">
        <ul class="social-likes" data-url="/2017/11/07/奇异值分解/">
          <li class="facebook" title="like"></li>
          <li class="twitter" title="twitte"></li>
          <li class="plusone" title="google"></li>
        </ul>
      </div>
    </div>
    
  </div>

  <div class="widgetarea">

    
    

<div class="tagcloud" id="fbox-tagcloud" style="margin:10px; display:none; overflow: hidden"><a href="/tags/Machine-Learning/" style="font-size: 12px;">Machine Learning</a></div>

    

    
      

<ul class="widget blogroll">
  <h3 class="title">Recent Post</h3>
  
    
    <li>
      <div class="text-holder">
        <h3 class="link-title"><a href="/2018/07/17/关于隐反馈的一些个人见解/">关于隐反馈的一些个人见解</a></h3>
        <p>关于隐反馈的一些个人见解显式反馈和隐式反馈的区别显式反馈中，矩阵中的每个元素1-5代表用户对物品的喜</p>
      </div>
    </li>
  
    
    <li>
      <div class="text-holder">
        <h3 class="link-title"><a href="/2017/12/08/集成学习下/">集成学习下</a></h3>
        <p>横跨快有两周的时间了，补一下集成学习下的内容。上上周写了集成学习上的博客，在那一块知识，有一个很典型</p>
      </div>
    </li>
  
    
    <li>
      <div class="text-holder">
        <h3 class="link-title"><a href="/2017/11/26/集成学习上/">集成学习上</a></h3>
        <p>周中在实验室seminar上做了一个有关集成学习的报告，趁周末有空，把集成学习的相关内容写个博客。我</p>
      </div>
    </li>
  
    
    <li>
      <div class="text-holder">
        <h3 class="link-title"><a href="/2017/11/07/奇异值分解/">奇异值分解</a></h3>
        <p>简介奇异值分解（Singular Value Decomposition，SVD）是矩阵分解的一种，</p>
      </div>
    </li>
  
    
    <li>
      <div class="text-holder">
        <h3 class="link-title"><a href="/2017/11/06/线性回归/">线性回归</a></h3>
        <p>线性回归线性回归是一种监督学习方法，可以被用来解决回归问题。它用一条直线或高维空间中的平面来拟合训练</p>
      </div>
    </li>
  
</ul>


    

  </div>
</div>


  </div>
  
<div class="footer">
  <div class="container">
    <a href="//github.com/kywk/hexo-theme-awe">AWE for hexo</a> inspired by <a href="http://goo.gl/H8OMRE">Awesome UI Kit</a>,
    ported by <a href="//kywk.github.io/">MooCow (Aka. kywk)</a>.
    
    &copy; 2018 Should Chan All Rights Reserved
    
  </div>
</div>

  <!-- disqus -->



<script>
$(document).ready(function() {
  $('.imgLiquid').imgLiquid({fill:true, fadeInTime:500});
});
</script>


<!-- fancybox -->
<link rel="stylesheet" href="/package/fancybox/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen" />
<script type="text/javascript" src="/package/fancybox/jquery.fancybox.pack.js?v=2.1.5"></script>
<script type="text/javascript">
(function($){
  $(".fancybox").fancybox();
  $(".various").fancybox({
    maxWidth  : 800,
    maxHeight : 600,
    fitToView : false,
    width   : '70%',
    height    : '70%',
    autoSize  : true,
    closeClick  : false,
    openEffect  : 'none',
    closeEffect : 'none'
  });
})(jQuery);
</script>

</body>
</html>
